# plugin-review Assertion-Driven Convergence Loop Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Refactor the plugin-review convergence loop so analyst subagents generate machine-verifiable assertions per finding, those assertions run automatically after every implementation pass, failures trigger a write-capable fix-agent, and the loop continues until all assertions pass or `--max-passes=N` is reached.

**Architecture:** Approach A — assertions are generated by existing analyst subagents (embedded in their output), collected by the orchestrator into `.claude/state/review-assertions.json`, and executed by a new bash script `scripts/run-assertions.sh`. A new `agents/fix-agent.md` (write-capable) handles targeted assertion-regression fixes. The Phase 4 human gate is removed; all proposals are auto-implemented.

**Tech Stack:** Bash (assertion runner), Python 3 (JSON processing in the script), Markdown (agents/commands/templates are behavioral instruction files, not compiled).

**Design doc:** `docs/plans/2026-02-20-plugin-review-assertion-loop-design.md`

---

## Implementation Order

1. Assertion runner script (testable bash component first)
2. Fix-agent (new agent file)
3. Analyst agent output format extensions (all three)
4. Orchestrator command refactor (core logic change)
5. Template updates (report format changes)
6. Version bump + CHANGELOG
7. Integration test against plugin-review itself

---

### Task 1: Create `scripts/run-assertions.sh`

**Files:**
- Create: `plugins/plugin-review/scripts/run-assertions.sh`
- Create (test): `plugins/plugin-review/scripts/test-run-assertions.sh`

**Step 1: Create the test fixture script**

Create `plugins/plugin-review/scripts/test-run-assertions.sh`:

```bash
#!/usr/bin/env bash
# Smoke-tests run-assertions.sh with synthetic assertions.
# Run from repo root: bash plugins/plugin-review/scripts/test-run-assertions.sh
set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
RUNNER="$SCRIPT_DIR/run-assertions.sh"
FIXTURE=$(mktemp)
trap 'rm -f "$FIXTURE"' EXIT

cat > "$FIXTURE" << 'EOF'
{
  "plugin": "test",
  "max_passes": 5,
  "current_pass": 1,
  "assertions": [
    {
      "id": "T-001",
      "finding_id": "test",
      "track": "A",
      "type": "grep_not_match",
      "description": "empty echo has no output",
      "command": "echo ''",
      "expected": "no_match",
      "status": null,
      "failure_output": null
    },
    {
      "id": "T-002",
      "finding_id": "test",
      "track": "A",
      "type": "grep_match",
      "description": "ls output is non-empty",
      "command": "ls /",
      "expected": "match",
      "status": null,
      "failure_output": null
    },
    {
      "id": "T-003",
      "finding_id": "test",
      "track": "C",
      "type": "file_exists",
      "description": "fixture file must exist",
      "path": "",
      "status": null,
      "failure_output": null
    },
    {
      "id": "T-004",
      "finding_id": "test",
      "track": "C",
      "type": "file_content",
      "description": "fixture contains plugin key",
      "path": "",
      "needle": "\"plugin\": \"test\"",
      "status": null,
      "failure_output": null
    },
    {
      "id": "T-005",
      "finding_id": "test",
      "track": "A",
      "type": "shell_exit_zero",
      "description": "true exits zero",
      "command": "true",
      "expected": "exit_zero",
      "status": null,
      "failure_output": null
    }
  ],
  "confidence": { "passed": 0, "total": 0, "score": 0.0 }
}
EOF

# Patch path-dependent assertions to use the fixture path
python3 -c "
import json, sys
data = json.load(open('$FIXTURE'))
for a in data['assertions']:
    if a['type'] in ('file_exists', 'file_content'):
        a['path'] = '$FIXTURE'
with open('$FIXTURE', 'w') as f:
    json.dump(data, f, indent=2)
"

echo "Running run-assertions.sh..."
ASSERTIONS_FILE="$FIXTURE" bash "$RUNNER"

echo ""
echo "Checking results..."
python3 -c "
import json, sys
data = json.load(open('$FIXTURE'))
errors = []
for a in data['assertions']:
    if a['status'] != 'pass':
        errors.append(f\"  {a['id']}: expected pass, got {a['status']} — {a.get('failure_output','')}\")
if errors:
    print('FAILURES:')
    print('\n'.join(errors))
    sys.exit(1)
if data['confidence']['score'] != 1.0:
    print(f\"Score wrong: {data['confidence']}\")
    sys.exit(1)
print(f\"All {len(data['assertions'])} assertions pass. Confidence: {int(data['confidence']['score']*100)}%\")
"
```

**Step 2: Run the test to verify it fails** (script doesn't exist yet)

```bash
bash plugins/plugin-review/scripts/test-run-assertions.sh
```

Expected: `bash: .../run-assertions.sh: No such file or directory`

**Step 3: Create `plugins/plugin-review/scripts/run-assertions.sh`**

```bash
#!/usr/bin/env bash
# Runs all assertions in ASSERTIONS_FILE. Consumed by commands/review.md Phase 5.5.
# Writes pass/fail status and confidence back to the same file.
# Exit code: 0 = all pass, 1 = some fail, 2 = script/file error.
# Environment: ASSERTIONS_FILE — path override (default: .claude/state/review-assertions.json).

set -uo pipefail

ASSERTIONS_FILE="${ASSERTIONS_FILE:-.claude/state/review-assertions.json}"

if [ ! -f "$ASSERTIONS_FILE" ]; then
  echo "Error: assertions file not found: $ASSERTIONS_FILE" >&2
  exit 2
fi

python3 << PYEOF
import json, subprocess, os, sys

assertions_file = os.environ.get('ASSERTIONS_FILE', '.claude/state/review-assertions.json')

with open(assertions_file) as f:
    data = json.load(f)

passed = 0
failed = 0

for a in data['assertions']:
    a_type = a['type']
    a_id = a['id']

    try:
        if a_type == 'grep_not_match':
            r = subprocess.run(a['command'], shell=True, capture_output=True, text=True)
            if r.stdout.strip() == '':
                a['status'] = 'pass'; a['failure_output'] = None; passed += 1
            else:
                a['status'] = 'fail'; a['failure_output'] = r.stdout.strip()[:500]; failed += 1

        elif a_type == 'grep_match':
            r = subprocess.run(a['command'], shell=True, capture_output=True, text=True)
            if r.stdout.strip() != '':
                a['status'] = 'pass'; a['failure_output'] = None; passed += 1
            else:
                a['status'] = 'fail'; a['failure_output'] = 'No output — pattern not found'; failed += 1

        elif a_type == 'file_exists':
            path = a.get('path', '')
            if os.path.exists(path):
                a['status'] = 'pass'; a['failure_output'] = None; passed += 1
            else:
                a['status'] = 'fail'; a['failure_output'] = f'Not found: {path}'; failed += 1

        elif a_type == 'file_content':
            path = a.get('path', '')
            needle = a.get('needle', '')
            if not os.path.exists(path):
                a['status'] = 'fail'; a['failure_output'] = f'File not found: {path}'; failed += 1
            elif needle in open(path).read():
                a['status'] = 'pass'; a['failure_output'] = None; passed += 1
            else:
                a['status'] = 'fail'; a['failure_output'] = f'Needle not found: {needle!r}'; failed += 1

        elif a_type == 'typescript_compile':
            r = subprocess.run(a['command'], shell=True, capture_output=True, text=True)
            out = (r.stdout + r.stderr).strip()
            if r.returncode == 0 and out == '':
                a['status'] = 'pass'; a['failure_output'] = None; passed += 1
            else:
                a['status'] = 'fail'; a['failure_output'] = out[:500]; failed += 1

        elif a_type == 'shell_exit_zero':
            r = subprocess.run(a['command'], shell=True, capture_output=True, text=True)
            if r.returncode == 0:
                a['status'] = 'pass'; a['failure_output'] = None; passed += 1
            else:
                out = (r.stdout + r.stderr).strip()
                a['status'] = 'fail'; a['failure_output'] = out[:500]; failed += 1

        else:
            a['status'] = 'fail'; a['failure_output'] = f'Unknown type: {a_type}'; failed += 1

    except Exception as e:
        a['status'] = 'fail'; a['failure_output'] = f'Runner error: {e}'; failed += 1

    icon = '✅' if a['status'] == 'pass' else '❌'
    print(f"  {icon} {a_id}: {a_type} — {a['description'][:60]}")
    if a['status'] == 'fail':
        print(f"     Failure: {(a.get('failure_output') or '')[:80]}")

total = len(data['assertions'])
data['confidence']['passed'] = passed
data['confidence']['total'] = total
data['confidence']['score'] = round(passed / total, 4) if total > 0 else 0.0

pct = int(data['confidence']['score'] * 100)
print(f"\nConfidence: {pct}% ({passed}/{total} assertions passing)")

with open(assertions_file, 'w') as f:
    json.dump(data, f, indent=2)

sys.exit(0 if failed == 0 else 1)
PYEOF
```

**Step 4: Make the script executable**

```bash
chmod +x plugins/plugin-review/scripts/run-assertions.sh plugins/plugin-review/scripts/test-run-assertions.sh
```

**Step 5: Run the test**

```bash
bash plugins/plugin-review/scripts/test-run-assertions.sh
```

Expected output:
```
Running run-assertions.sh...
  ✅ T-001: grep_not_match — empty echo has no output
  ✅ T-002: grep_match — ls output is non-empty
  ✅ T-003: file_exists — fixture file must exist
  ✅ T-004: file_content — fixture contains plugin key
  ✅ T-005: shell_exit_zero — true exits zero

Confidence: 100% (5/5 assertions passing)
Checking results...
All 5 assertions pass. Confidence: 100%
```

**Step 6: Test a failing assertion case**

Add to test script (temporarily) an assertion that should fail. Run to confirm `status: fail` and exit code 1. Then remove it.

```bash
# Quick inline check:
ASSERTIONS_FILE=/tmp/fail-test.json python3 -c "
import json
data = {'plugin':'t','max_passes':1,'current_pass':1,'assertions':[{
  'id':'F-001','finding_id':'x','track':'A','type':'file_exists',
  'description':'missing file','path':'/nonexistent/path',
  'status':None,'failure_output':None
}],'confidence':{'passed':0,'total':0,'score':0.0}}
json.dump(data, open('/tmp/fail-test.json','w'), indent=2)
"
ASSERTIONS_FILE=/tmp/fail-test.json bash plugins/plugin-review/scripts/run-assertions.sh; echo "Exit: $?"
```

Expected: `❌ F-001` and `Exit: 1`

**Step 7: Commit**

```bash
git add plugins/plugin-review/scripts/run-assertions.sh plugins/plugin-review/scripts/test-run-assertions.sh
git commit -m "feat(plugin-review): add run-assertions.sh with smoke tests"
```

---

### Task 2: Create `agents/fix-agent.md`

**Files:**
- Create: `plugins/plugin-review/agents/fix-agent.md`

**Step 1: Create the file**

```markdown
---
name: fix-agent
description: Targeted implementation agent for assertion-driven fixes. Receives a list of
  failing assertions with context and implements the minimal fix for each. Called by the
  orchestrator after run-assertions.sh finds failures.
tools: Read, Grep, Glob, Edit, Write
---

# Agent: Fix-Agent

<!-- architectural-context
  Role: write-capable targeted fixer invoked by commands/review.md Phase 5.5 when
    run-assertions.sh finds failing assertions after the main implementation pass.
  Contrast with analyst subagents (principles-analyst, ux-analyst, docs-analyst) which
    are read-only (tools: Read, Grep, Glob). This is the ONLY write-capable subagent;
    all other subagents delegate implementation to the orchestrator.
  Input contract: the orchestrator provides a list of failing assertion objects and the
    original analyst findings that generated them. The agent reads context, fixes, returns
    a structured summary.
  Output contract: "## Fix-Agent Results — Pass N" section with one sub-section per
    assertion ID. The orchestrator embeds this in the pass report.
  What breaks if this changes: the orchestrator's Phase 5.5 instruction references this
    agent by name and expects the structured output format below.
-->

You are a targeted fix implementation agent. Your sole job is to make failing assertions
pass by implementing the minimum necessary change. You do not analyze broadly, refactor,
or implement changes beyond what the assertion explicitly requires.

## Role Boundaries

**You may:** Read files, implement targeted fixes, write minimal changes to make assertions pass.
**You may not:** Refactor unrelated code, add features, address non-failing assertions,
interact with the user.

## Input

The orchestrator provides:
1. A list of failing assertion objects (id, type, command/path, description, failure_output)
2. The original analyst finding each assertion was generated from (for context)
3. Which files are likely relevant for each fix

## Process

For each failing assertion:
1. Read the relevant files to understand the current state
2. Determine the minimal change needed to make the assertion pass
3. Implement the change
4. Return a one-line summary of what changed

**Scope discipline:** Fix only what the assertion requires. If fixing one assertion would
naturally fix another, note it — do not expand scope without noting the overlap.

## Output Format

```
## Fix-Agent Results — Pass <N>

### <assertion-id> — <description>
Changed: <file-path>:<line-range> — <what changed in one sentence>

### <assertion-id> — <description>
Changed: <file-path>:<line-range> — <what changed in one sentence>

### Summary
Fixed: <N> assertions | Unchanged: <N> (already passing or out of scope)
```

If an assertion cannot be fixed (the required change is architectural or out of scope),
report it as:
```
### <assertion-id> — <description>
Unresolvable: <reason in one sentence>
```

Do not deviate from this format. The orchestrator embeds this summary in the pass report.
```

**Step 2: Verify frontmatter is valid**

```bash
head -6 plugins/plugin-review/agents/fix-agent.md
```

Expected:
```
---
name: fix-agent
description: Targeted implementation agent...
tools: Read, Grep, Glob, Edit, Write
---
```

**Step 3: Commit**

```bash
git add plugins/plugin-review/agents/fix-agent.md
git commit -m "feat(plugin-review): add fix-agent for assertion-driven targeted fixes"
```

---

### Task 3: Update `agents/principles-analyst.md` — Add Assertions block

**Files:**
- Modify: `plugins/plugin-review/agents/principles-analyst.md`

**Step 1: Add the Assertions section to the output format**

After the closing ` ``` ` of the current output format block, add:

```markdown
## Assertions Output

After your findings, append an `## Assertions` section containing a JSON array of
machine-verifiable checks, one per open finding:

\```
## Assertions

\```json
[
  {
    "id": "A-<track>-<number>",
    "finding_id": "<principle or checkpoint ID, e.g. P3 or C1>",
    "track": "A",
    "type": "<grep_not_match | grep_match | file_exists | file_content | typescript_compile | shell_exit_zero>",
    "description": "One sentence: what this assertion verifies",
    "command": "<bash command to run — use full relative paths from repo root>",
    "expected": "<no_match | match | exists | contains | no_output | exit_zero>",
    "path": "<file path — only for file_exists and file_content types>",
    "needle": "<search string — only for file_content type>"
  }
]
\```
\```

**Assertion type guide:**
- `grep_not_match`: the finding is a pattern that should NOT appear (e.g., banned keyword, disallowed construct). Command should grep for the bad pattern; expect empty output.
- `grep_match`: the finding is a pattern that SHOULD appear (e.g., required comment header). Command greps for it; expect non-empty output.
- `file_exists`: the finding is a missing file. Use `path` field (no `command`).
- `file_content`: the finding is missing content in an existing file. Use `path` + `needle` fields.
- `typescript_compile`: the finding is a TypeScript type error. Command should be `cd <dir> && npx tsc --noEmit 2>&1`. Only use when target plugin has TypeScript source.
- `shell_exit_zero`: the finding is a script that should run cleanly. Command is the test invocation.

**Write one assertion per open finding.** If a finding has no machine-verifiable check (e.g., pure judgment calls), omit it — do not invent synthetic assertions. Assertions for upheld principles should NOT be included (only open findings get assertions).

Do not include assertions for upheld/clean items. Only include assertions that will currently FAIL (because the finding represents a current gap).
```

**Step 2: Verify the file structure is intact**

```bash
head -10 plugins/plugin-review/agents/principles-analyst.md
```

Expected: frontmatter unchanged.

**Step 3: Commit**

```bash
git add plugins/plugin-review/agents/principles-analyst.md
git commit -m "feat(plugin-review): add Assertions block to principles-analyst output format"
```

---

### Task 4: Update `agents/ux-analyst.md` — Add Assertions block

**Files:**
- Modify: `plugins/plugin-review/agents/ux-analyst.md`

**Step 1: Add the same Assertions Output section** (same content as Task 3) after the closing ` ``` ` of the output format block. Replace `"track": "A"` with `"track": "B"` and update the ID prefix to `"id": "A-B-<number>"`.

Paste the same Assertions Output section from Task 3, but change:
- `"track": "A"` → `"track": "B"`
- `"id": "A-<track>-<number>"` → `"id": "A-B-<number>"`

Also update the guidance to be UX-specific:
```
**UX assertion guidance:**
- UX findings typically yield `grep_not_match` (pattern that indicates bad UX, e.g., an open-ended prompt instead of AskUserQuestion) or `grep_match` (required pattern, e.g., structured output keyword).
- For touchpoint violations, the command should grep the file containing the touchpoint.
```

**Step 2: Verify frontmatter**

```bash
head -6 plugins/plugin-review/agents/ux-analyst.md
```

**Step 3: Commit**

```bash
git add plugins/plugin-review/agents/ux-analyst.md
git commit -m "feat(plugin-review): add Assertions block to ux-analyst output format"
```

---

### Task 5: Update `agents/docs-analyst.md` — Add Assertions block

**Files:**
- Modify: `plugins/plugin-review/agents/docs-analyst.md`

**Step 1: Add the Assertions Output section** (same pattern, track C). Unique docs guidance:

```
**Docs assertion guidance:**
- Stale doc findings: `file_content` with the needle being the updated content that should be present.
- Missing doc findings: `file_exists` for the file that should exist.
- Orphaned reference findings: `grep_not_match` for the dead reference string.
- Doc accuracy findings (implementation says X but doc says Y): `grep_match` checking the doc for the correct content.
```

**Step 2: Verify frontmatter**

```bash
head -6 plugins/plugin-review/agents/docs-analyst.md
```

**Step 3: Commit**

```bash
git add plugins/plugin-review/agents/docs-analyst.md
git commit -m "feat(plugin-review): add Assertions block to docs-analyst output format"
```

---

### Task 6: Refactor `commands/review.md` — Core Orchestrator Changes

This is the largest change. Apply the following modifications in order.

**Files:**
- Modify: `plugins/plugin-review/commands/review.md`

**Step 1: Read the full current file to understand context**

Read `plugins/plugin-review/commands/review.md` in full before making any edits.

**Step 2: Update Phase 1 — add `--max-passes` parsing and assertions file init**

In the Phase 1 Setup bash block (the one that sets `PLUGIN_REVIEW_ACTIVE=1`), extend it to:

```bash
export PLUGIN_REVIEW_ACTIVE=1
mkdir -p .claude/state

# Parse --max-passes=N from the user's invocation (default 5)
# Extract from the trigger text: "review plugin-name --max-passes=7" → MAX_PASSES=7
MAX_PASSES=5  # replace with parsed value from invocation text, or keep 5

echo "{\"impl_files\":[],\"doc_files\":[],\"pass_number\":1,\"max_passes\":$MAX_PASSES}" > .claude/state/plugin-review-writes.json
echo "{\"plugin\":\"\",\"max_passes\":$MAX_PASSES,\"current_pass\":1,\"assertions\":[],\"confidence\":{\"passed\":0,\"total\":0,\"score\":0.0}}" > .claude/state/review-assertions.json
echo "✓ Plugin review session activated (max passes: $MAX_PASSES)"
```

Also add to the **Behavior** intro paragraph (before Phase 1), one sentence:

```
Parse `--max-passes=N` from the user's invocation using a regex match on `--max-passes=(\d+)`; default to 5 if not present. This becomes the loop safety limit replacing the old 3-pass budget.
```

**Step 3: Add Phase 2.5 — Assertion Collection**

After the Phase 2 paragraph (the one ending with "Carry forward unchanged findings"), add a new section:

```markdown
### Phase 2.5 — Assertion Collection

Each analyst's output contains an `## Assertions` block with a JSON array. Extract each array and merge all assertions into `.claude/state/review-assertions.json`:

1. Read `.claude/state/review-assertions.json`
2. Set `plugin` to the current plugin name and `current_pass` to the current pass number
3. For each assertion from each analyst, add it to the `assertions` array **only if its `id` is not already present** — this prevents duplicate assertions on re-audit passes
4. Write the updated file

After merging, report the assertion count: "Pass N: N total assertions (N new, N carried forward)."
```

**Step 4: Rewrite Phase 4 — Remove human gate, auto-implement**

Replace the current Phase 4 text entirely with:

```markdown
### Phase 4 — Auto-Implement All Proposals

For each open finding, propose and immediately implement a concrete fix. Do not use `AskUserQuestion` — all proposals are auto-implemented.

For each fix:
1. State the plan — files, changes, gap closure.
2. Load `<CLAUDE_PLUGIN_ROOT>/templates/cross-track-impact.md` and note which other tracks are affected.
3. Implement the code change.
4. Update documentation — identify any docs referencing the modified behavior and update them.
5. Summarize in 1–2 sentences.

If zero open findings remain, skip to Phase 5.5 (run assertions to verify).
```

**Step 5: Extend Phase 5 — Add assertion runner and fix-agent loop**

After the current Phase 5 text (which ends with "loop back to Phase 2"), replace the last paragraph with:

```markdown
After all changes, increment `pass_number`:

```bash
python3 -c "
import json
d = json.load(open('.claude/state/plugin-review-writes.json'))
d['pass_number'] = d.get('pass_number', 1) + 1
json.dump(d, open('.claude/state/plugin-review-writes.json', 'w'), indent=2)
"
```

### Phase 5.5 — Run Assertions

Run the full assertion suite:

```bash
bash $CLAUDE_PLUGIN_ROOT/scripts/run-assertions.sh
```

Read the updated confidence score and failing assertions:

```bash
python3 -c "
import json
d = json.load(open('.claude/state/review-assertions.json'))
pct = int(d['confidence']['score'] * 100)
print(f\"Confidence: {pct}% ({d['confidence']['passed']}/{d['confidence']['total']})\")
fails = [a for a in d['assertions'] if a['status'] == 'fail']
for a in fails:
    print(f\"  ❌ {a['id']} ({a['track']}): {a['description']}\")
    if a.get('failure_output'):
        print(f\"     {a['failure_output'][:100]}\")
"
```

**If confidence is 100%**, proceed to Phase 6 (convergence).

**If any assertions fail**, spawn the fix-agent (`agents/fix-agent.md`) with:
- The list of failing assertion objects (full JSON from `.claude/state/review-assertions.json` filtered to `status == "fail"`)
- The original analyst finding context for each (from the analyst summaries collected in Phase 2)
- The specific files likely needing changes per assertion

After the fix-agent returns, re-run assertions and update confidence:

```bash
bash $CLAUDE_PLUGIN_ROOT/scripts/run-assertions.sh
```

Read and report the updated confidence.

**Check max-passes budget:**

```bash
python3 -c "
import json
writes = json.load(open('.claude/state/plugin-review-writes.json'))
review = json.load(open('.claude/state/review-assertions.json'))
pass_num = writes.get('pass_number', 1)
max_passes = writes.get('max_passes', 5)
pct = int(review['confidence']['score'] * 100)
print(f'Pass {pass_num}/{max_passes} — Confidence: {pct}%')
if pass_num >= max_passes and review['confidence']['score'] < 1.0:
    fails = [a['id'] for a in review['assertions'] if a['status'] == 'fail']
    print(f'BUDGET_REACHED: {len(fails)} assertions still failing: {fails}')
    print('Proceeding to Phase 6 (budget stop).')
"
```

If `pass_number >= max_passes` and confidence < 100%, proceed to Phase 6 with convergence reason "Budget reached."

Otherwise, if open findings remain from the analyst reports, loop back to Phase 2 (scoped re-audit).
```

**Step 6: Update Phase 6 — Add confidence score to final report**

Add to the Phase 6 section, before the `unset PLUGIN_REVIEW_ACTIVE` block:

```markdown
Read the final confidence score and include it in the final report:

```bash
python3 -c "
import json
d = json.load(open('.claude/state/review-assertions.json'))
pct = int(d['confidence']['score'] * 100)
total = d['confidence']['total']
passed = d['confidence']['passed']
print(f'Final confidence: {pct}% ({passed}/{total} assertions passing)')
print(f'Assertion summary:')
for a in d['assertions']:
    icon = '✅' if a['status'] == 'pass' else '❌'
    print(f'  {icon} {a[\"id\"]} ({a[\"track\"]}): {a[\"description\"]}')
"
```

Include confidence in the final report output (see `templates/final-report.md` format).
```

Also add `review-assertions.json` cleanup to the session cleanup block:

```bash
unset PLUGIN_REVIEW_ACTIVE
rm -f .claude/state/plugin-review-writes.json .claude/state/review-assertions.json
echo "✓ Plugin review session ended"
```

**Step 7: Update the Hard Rules section**

Remove the old 3-pass budget rule and replace with:

```
- Respect the `max_passes` budget (default 5). Report confidence when budget is reached.
- Do NOT use AskUserQuestion during the review loop — the loop is fully automated.
- Phase 4 is auto-implement. No human approval gates anywhere in the loop.
```

**Step 8: Verify the file looks correct**

Read `plugins/plugin-review/commands/review.md` in full and check:
- Phase 1 has `--max-passes` parsing and assertions file init
- Phase 2.5 exists with assertion collection instructions
- Phase 4 has no `AskUserQuestion`
- Phase 5.5 exists with assertion runner, fix-agent spawn, and budget check
- Phase 6 has confidence reporting and `review-assertions.json` cleanup

**Step 9: Commit**

```bash
git add plugins/plugin-review/commands/review.md
git commit -m "feat(plugin-review): refactor convergence loop — auto-implement, assertion runner, fix-agent"
```

---

### Task 7: Update `templates/pass-report.md` — Add Confidence Column

**Files:**
- Modify: `plugins/plugin-review/templates/pass-report.md`

**Step 1: Update the convergence table header in both the Pass 1 and Pass 2+ formats**

In the Pass 1 format, replace:
```
| Pass | Upheld | Partial | Violated | Checkpoints | UX Issues | Stale Docs | Trend |
|------|--------|---------|----------|-------------|-----------|------------|-------|
| 1    | ...    | ...     | ...      | ...         | ...       | ...        | —     |
```
With:
```
| Pass | Upheld | Partial | Violated | Checkpoints | UX Issues | Stale Docs | Confidence | Trend |
|------|--------|---------|----------|-------------|-----------|------------|------------|-------|
| 1    | ...    | ...     | ...      | ...         | ...       | ...        | N%         | —     |
```

In the Pass 2+ format, update the table header to match.

**Step 2: Commit**

```bash
git add plugins/plugin-review/templates/pass-report.md
git commit -m "feat(plugin-review): add Confidence column to pass report convergence table"
```

---

### Task 8: Update `templates/final-report.md` — Add Assertions Section

**Files:**
- Modify: `plugins/plugin-review/templates/final-report.md`

**Step 1: Add Assertions Coverage section**

After the `### Documentation Status` table and before `### Accepted Gaps`, add:

```markdown
### Assertion Coverage
Confidence: N% (N/N assertions passing)
(omit if no assertions were generated — target plugin had no machine-verifiable findings)

| ID    | Track | Type             | Status | Description                        |
|-------|-------|------------------|--------|------------------------------------|
| A-001 | A     | grep_not_match   | ✅     | <description>                      |
| A-002 | C     | file_content     | ❌     | <description> — <failure note>     |
```

**Step 2: Update the Rules section** to add:

```
Assertion Coverage is included when assertions were generated. If zero assertions were generated (all findings were judgment-only), omit the section and note "No machine-verifiable assertions generated."
```

**Step 3: Commit**

```bash
git add plugins/plugin-review/templates/final-report.md
git commit -m "feat(plugin-review): add Assertions Coverage section to final report"
```

---

### Task 9: Version Bump + CHANGELOG

**Files:**
- Modify: `plugins/plugin-review/.claude-plugin/plugin.json`
- Modify: `plugins/plugin-review/CHANGELOG.md`
- Modify: `.claude-plugin/marketplace.json`

**Step 1: Bump version in plugin.json to 0.3.0**

```json
{
  "name": "plugin-review",
  "description": "Comprehensive plugin review covering principles alignment, terminal UX quality, and documentation freshness via orchestrator-subagent architecture.",
  "version": "0.3.0",
  "author": {
    "name": "L3DigitalNet",
    "url": "https://github.com/L3DigitalNet"
  }
}
```

**Step 2: Add CHANGELOG entry**

Prepend to `CHANGELOG.md` (before the `## [0.2.0]` line):

```markdown
## [0.3.0] - 2026-02-20

### Added
- `scripts/run-assertions.sh` — machine-verifiable assertion runner; reads `.review-assertions.json`, executes all assertions by type (grep_not_match, grep_match, file_exists, file_content, typescript_compile, shell_exit_zero), updates pass/fail status, computes confidence score
- `scripts/test-run-assertions.sh` — smoke test for the assertion runner
- `agents/fix-agent.md` — write-capable targeted fix agent for assertion-driven regressions; one invocation per pass, receives all failing assertions and implements minimal fixes
- `## Assertions` output block to all three analyst agents (principles-analyst, ux-analyst, docs-analyst) — each analyst generates machine-verifiable JSON assertions alongside findings
- Phase 2.5 (Assertion Collection) to orchestrator — merges analyst assertion blocks into `.review-assertions.json` state file
- Phase 5.5 (Assertion Runner) to orchestrator — runs full assertion suite after implementation, spawns fix-agent for failures, reports confidence
- `--max-passes=N` flag — parsed from invocation text; replaces hardcoded 3-pass budget (default 5)
- Confidence score (`assertions_passed / total_assertions`) reported at each pass and in final report
- `Confidence` column in pass-report.md convergence table
- Assertion Coverage section in final-report.md

### Changed
- Phase 4 is now fully automated — `AskUserQuestion` gate removed; all proposals auto-implemented
- Pass budget changed from hardcoded 3 to `--max-passes=N` (default 5)
- Session cleanup now removes both `plugin-review-writes.json` and `review-assertions.json`
- State file initialization includes `max_passes` field

### Fixed
- CHANGELOG duplicate `### Added` section header in 0.2.0 entry (pre-existing drift)
```

**Step 3: Bump version in marketplace.json**

In `.claude-plugin/marketplace.json`, find the `plugin-review` entry and update `"version": "0.2.0"` to `"version": "0.3.0"`.

**Step 4: Validate marketplace**

```bash
./scripts/validate-marketplace.sh
```

Expected: all plugins valid, no errors.

**Step 5: Commit**

```bash
git add plugins/plugin-review/.claude-plugin/plugin.json plugins/plugin-review/CHANGELOG.md .claude-plugin/marketplace.json
git commit -m "chore(plugin-review): bump to v0.3.0 — assertion-driven convergence loop"
```

---

### Task 10: Integration Test

**Step 1: Run the assertion script test to confirm everything still works**

```bash
bash plugins/plugin-review/scripts/test-run-assertions.sh
```

Expected: all assertions pass.

**Step 2: Run plugin-review against itself**

```
/review plugin-review --max-passes=3
```

Observe:
- The loop runs without human gates
- Analysts generate `## Assertions` blocks
- Phase 2.5 collects assertions into `.claude/state/review-assertions.json`
- Phase 5.5 runs assertions and reports confidence
- Any failures trigger the fix-agent
- Final report includes confidence score

**Step 3: Verify `.review-assertions.json` is populated**

```bash
python3 -c "
import json
d = json.load(open('.claude/state/review-assertions.json'))
print(f'Assertions generated: {len(d[\"assertions\"])}')
for a in d['assertions']:
    print(f'  {a[\"id\"]} ({a[\"track\"]}): {a[\"type\"]} — {a[\"description\"][:50]}')
print(f'Confidence: {int(d[\"confidence\"][\"score\"]*100)}%')
"
```

Expected: multiple assertions generated, confidence > 0%.

**Step 4: Commit any fixes surfaced during integration test**

If the integration test surfaces real issues in the plugin itself (that the review correctly identifies), commit those fixes as a separate commit with a clear message.

**Step 5: Final commit**

```bash
git add -p  # review staged changes
git commit -m "test(plugin-review): integration test against plugin-review — assertion loop working"
```

---

## Sequence Summary

| Task | File(s) | Commit |
|------|---------|--------|
| 1 | `scripts/run-assertions.sh` + test | `feat: add run-assertions.sh with smoke tests` |
| 2 | `agents/fix-agent.md` | `feat: add fix-agent` |
| 3 | `agents/principles-analyst.md` | `feat: add Assertions block to principles-analyst` |
| 4 | `agents/ux-analyst.md` | `feat: add Assertions block to ux-analyst` |
| 5 | `agents/docs-analyst.md` | `feat: add Assertions block to docs-analyst` |
| 6 | `commands/review.md` | `feat: refactor convergence loop — auto-implement + assertion runner` |
| 7 | `templates/pass-report.md` | `feat: add Confidence column` |
| 8 | `templates/final-report.md` | `feat: add Assertions Coverage section` |
| 9 | version + CHANGELOG + marketplace | `chore: bump to v0.3.0` |
| 10 | integration test | `test: integration test passes` |
